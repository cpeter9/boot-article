\documentclass[twoside]{article}

\usepackage[round]{natbib}

\usepackage{amsmath}
\makeatletter
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%If there's more than a single symbol, use the first character instead (see below):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = (italic correction / 3) - (Breite / 10)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern (-\dimen@) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother
\newcommand\test[1]{%
$#1{M}$ $#1{A}$ $#1{g}$ $#1{\beta}$ $#1{\mathcal A}^q$
$#1{AB}^\sigma$ $#1{H}^C$ $#1{\sin z}$ $#1{W}_n$}

% \bibpunct{(}{)}{;}{a}{}{,}


\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
% \renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\arabic{section}}
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{On Solving Applied Inference Problems Using Bootstrap and Simulation Models} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

%----------------------------------------------------------------------------------------
%  TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{On Solving Applied Inference Problems Using Bootstrap and Simulation Models}} % Article title

\author{
\large
\textsc{Christopher Peters}\thanks{Christopher P. Peters is Data Scientist, Treehouse Inc., 622 E. Washington Street Suite 240, Orlando, Florida 32801, USA and Master's of Applied Statistics student, Louisiana State University, 161 Martin D. Woodin Hall, LSU Baton Rouge, Louisiana 70803}\\[2mm] % Your name
\normalsize Louisiana State University \\ % Your institution
\normalsize \href{mailto:cpeter9@gmail.com}{cpeter9@gmail.com} % Your email address
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------



<<set-options, include=FALSE, cache=FALSE>>=
opts_chunk$set(dev='tikz', fig.align='center', cache=TRUE, message=FALSE, background='white')
options(replace.assign=TRUE,width=85)
knit_hooks$set(fig=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)})
library(ggplot2)
theme_set(theme_grey(base_size = 9))
@


\begin{document}


\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%  ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}

\noindent Much of what young statisticians in training learn in undergraduate and graduate school relies on large-sample asymptotic theory.  Well known statistics are calculated and with a few assumptions a standard error is developed to measure inference reliability.  Unfortunately data are not always sufficiently large such that finite sample properties match those required by large-sample asymptotics.  Additionally a careful examination of assumptions is always required for successful inference.  In some cases common checks of assumptions may not be possible with a high degree of power, especially in cases of  small samples.  One common solution to this problem involves resampling data with replacement to empirically calculate the variance of any statistic.  This solution is generally referred to as bootstrapping.  In addition to its uses for inference, variations on boostrapping methods can be used to simulate data for cross-validation or other purposes.  This article will focus on three applications of boostrapping.  The first is verification of power calculations for a simple experiment.  The second involves inference in the presence of heteroskedasticity.  The third focuses on bootstrapping for cross-validation.

\end{abstract}

%----------------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Verification of power calculations}

Power calculations are generally considered good pre-requisite for any designed experiment.  Such calculations allow researchers to determine reasonable sample size requirements before carrying out an experiment.  Such calculations can help researchers avoid carrying out experiments doomed in the sense of statistical detection before experimentation begins.\\ 

The following example takes a look at how power calculations can be carried out and verified by bootstrapping sample data.  In this case a web app company wants to perform an experiment on a business practice.  The company has found a relationship between earning tokens on the site and customer longevity and financial value.  The company decides that it would like to encourage its customers to earn more tokens by sending an email.  However, the company would also like to compare the effect of the email against a control group of customers that did not receive the email.  A two-sample t-test is selected to test for a difference between the mean number of tokens earned in the experiment group and control group.\\

A major assumption underlying the two-sample t-test is that the two samples from come from normally distributed populations.  When this assumption is violated, distribution-free tests such as the Wilcoxon-Mann-Whitney (WMW or Mann-Witney U) test are commonly 
used \citep{skovlund01}. It is well known that the two-sample t-test is robust to departures from normality when data are large, but how large?\\

Suppose that we collect data on token earning prior to our experiment.  In this case the data from two randomly selected groups looks as is shown in Figure~\ref{fig:prob_1_raw_data} in Appendix A.  The data are clearly right skewed and look F, Chi-square, or exponentially distributed. While the t-test is usually robust to departures from normality in large samples, it is costly for the company to run large experiments.  Using bootstrapping, we can resample data from the same data stream that will record the effect of the experiment.  This will allow us to take thousands of samples for varying samples sizes in order to compare power and Type I error rates between the two methods.  The two-sample t-test with pooled variances and with separate variances is also analyzed.  By bootstrapping we are granted a look at the robustness of the two-sample t-test for this data with respect to sample size and when it should be preferred to the WMW test.\\

The standard two-sample t-test is shown below:

\begin{align}
t &= \frac{\widebar{Y_1} - \widebar{Y_2}}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
\end{align}

The WMW statistic is calculated as follow:

\begin{align}
W &= \frac{\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}I(x_i, y_j)}{mn} && \text{where}\\
I(x_i, y_i) &=
  \begin{array}{l}
    1 : x_i > y_j,\\
    0 : otherwise
  \end{array}
\end{align}

The {\it W} statistic as formulated above is an estimator of P$\left[X > Y\right]$.\\


In order to assess power and type 1 error rates by sample size:
\begin{enumerate}
\item Take two random samples of the target data with replacement with for sample sizes $10\ldots 1010$ by 10.
\item Calculate p-value and store
\item Repeat steps 1 and 2 {\it B} times.  In this case 200 bootstrap samples were taken for each sample and 200 p-values were calculated.
\end{enumerate}

To prove the method, Figure~\ref{type1error} shows type 1 error rate for two random samples from a normal distribution with mean $=$ 4 and standard deviations of 1 and 3, respectively for sample a and sample b bootstraps.  The error rates for the t-test procedure assuming equal variances is exactly the same as that not assuming equal variances.  These error rates are centered around 0.05 as expected.  However, interestingly, these error rates do not appear to be exact nor depend on sample size.  The WMW test maintains an almost universally higher Type 1 error rate compared to the two t-tests except in samples of less than 30.\\

Power can also be assessed by boostrapping.  The bootstrapping procedure was modified to select two random samples: one from a normal distribution with mean of 4 and standard deviation of 1, and a second sample from a normal distribution with mean 4.4 (10\% higher) and a standard deviation of 3.  Figure~\ref{prob_1_power} shows the share of bootstrapped samples for which the null hypothesis of no difference was rejected for each test by sample size.  Power is low for all tests starting with a sample size of 10.  As the selected sample sizes grow, power increases for all three tests with both variations of the t-test showing identical power.  As sample sizes grow larger than 100, t-tests noticably out perform the WMW test.  Interestingly, power between the t-tests and the WMW test appears to converge around sample size 900.\\

Finally, both bootstrapping analyses are performed on the actual token earning data from the company.  Since this data is not normally distributed, it violates a necessary condition for using t-tests in small samples.  Figure~\ref{raw_data_type_1} shows type 1 error rates for each test applied to 200 bootstrap samples at each sample size.  Both t-tests exhibit lower type 1 error rates in samples smaller than about 300 observations compared to the WMW test.  However as sample size increases the WMW test exhibits lower type 1 error rates compared to the two t-tests.  Also, the t-test respecting unequal variances under performs that assuming equal variances in samples of less than about 110 observations.\\

Power calculations were also performed for all three test variations on the company's data.  Token data was randomly selected for each sample group.  One sample group was left as-is while the other was multiplied by 1.1 to increase the mean 10\%.  This level of increase is motivated by a meaningful difference in token earning as suggested by company representatives.  The WMW test vastly outperformed the two-sample t-tests.  The two variations of t-test performed identically.  By sample size of about 250 observations, the WMW test rejects the null hypothesis in 100\% of the bootstrap samples.  However, both t-tests have only rejected the null hypothesis about 15\% of the time.

% 
% The results in Figure~\ref{prob_1_data} in Appendix A show that the mean differences are indeed normally distributed.  The histogram of mean differences appears to visually approximate a normal distribution.  Increasing the number of bootstrap samples taken causes the histogram to converge to normal with no recognizable difference.  Therefore, the assumption of normality is met in this case of two-sample t-test.\\
% 
% Many statistical tools such as analysis of variance and linear regression require assumptions of normality.  If these assumptions are not met these tools are not valid and may be biased without further transformation of data or adjustments to standard errors.  Bootstrapping is a simple way, often overlooked, to check distributional assumptions.  The general procedure is simple:\\
% 
% \begin{enumerate}
% \item Randomly sample data of interest with replacement B times (generally 1,000 samples is enough).
% \item Calculate and save the statistic of interest for each sample.
% \item Look at the histogram of that statistic.  If it's normally distributed, calculate the standard deviation of the statistic across all samples to obtain the standard error of the statistic.
% \end{enumerate}
% 
% Another assumption requred for the two-sample t-test to be valid is that the standard deviation of the differences is $\chi^{2}$ distributed with posi
% 
% One simple tool for power calculations is the~\href{http://cran.r-project.org/web/packages/pwr/index.html}{pwr} package found in the R language.



\section{Using bootstrapping to test for normality in small samples}

Normality is a common assumption across many statistical procedures such as Analysis of Variance (ANOVA), linear regression, t-tests and z-tests.  When normality assumptions are violated, statistics that make up the aformentioned methods are not valid and may leader to incorrect inference.  There are two main ways that normality is commonly assessed: graphical methods, including histograms plotted against densitites and Q-Q plots.  The second method invovles the use of one of a number of statistical tests for normality.\\

Small sample sizes typically pose problems for statistical tests of normality.  The major challenge is that such tests have lowre power in small samples and also that small samples may exhibit properities that fool tests for normlaity.  Type 1 error rates, that is rejecting the null hypothesis that the sample data were collected from a population with a normal distribution, may become unstable in small sample sizes.\\

This paper evaluates four major statistical tests for normality.  The tests include the Shapiro-Wilk test, Kolmogorov-Smirnov test, the Anderson-Darling test and Jarque-Bera test.  The analysis relies on monte carlo simulation as well as bootstrapping across sample sizes.

\subsection{Tests for Normality}


\section{Bootstrapping for cross-validation}


\bibliography{boot_biblio}{}
\bibliographystyle{plainnat}

\newpage
\section{Appendix A}

\begin{figure}[h!]
<<fig_1, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='3in', out.height='3in', cache=FALSE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")


sample_a <- sample(table(data$user_id), 400, replace = TRUE)
sample_b <- sample(table(data$user_id), 400, replace = TRUE)

raw.samples <- as.data.frame(t(rbind(sample_a, sample_b)))
raw.samples <- melt(raw.samples)

ggplot(raw.samples, aes(x = value)) +
  geom_histogram(aes(y = ..density..)) +
  facet_grid(variable ~ . )
@
\caption{test caption}
\label{fig:prob_1_raw_data}
\end{figure}

\begin{figure}[h!]
<<type1error, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j
for(i in 1:B){
  
  sample_a <- rnorm(sample.size, mean = 4, sd = 1)
  sample_b <- rnorm(sample.size, mean = 4, sd = 3)
  
  sample_c <- rnorm(sample.size, mean = 4, sd = 1)
  sample_d <- rnorm(sample.size, mean = 4 * 1.1, sd = 3) # thirty percent increase in mean

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
  
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test = t_type_1, t_test_unequal = t_unequal_type_1, wmw_test = wmw_type_1))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{type1error}
\end{figure}


\begin{figure}[h!]
<<power, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j
for(i in 1:B){
  
  sample_a <- rnorm(sample.size, mean = 4, sd = 1)
  sample_b <- rnorm(sample.size, mean = 4, sd = 3)
  
  sample_c <- rnorm(sample.size, mean = 4, sd = 1)
  sample_d <- rnorm(sample.size, mean = 4 * 1.1, sd = 3) # thirty percent increase in mean

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
  
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test_pwr = t_pwr, t_test_unequal_pwr = t_unequal_pwr, wmw_test_pwr = wmw_pwr))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{prob_1_power}
\end{figure}

\begin{figure}[h!]
<<raw_data_type_1, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j 
for(i in 1:B){
  
  sample_a <- sample(table(data$user_id), sample.size, replace = TRUE)
  sample_b <- sample(table(data$user_id), sample.size, replace = TRUE)

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test = t_type_1, t_test_unequal = t_unequal_type_1, wmw_test = wmw_type_1))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{raw_data_type_1}
\end{figure}

\begin{figure}[h!]
<<raw_data_power, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j 
for(i in 1:B){
  
  sample_c <- sample(table(data$user_id), sample.size, replace = TRUE)
  sample_d <- sample(table(data$user_id), sample.size, replace = TRUE) * 1.1
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test_pwr = t_pwr, t_test_unequal_pwr = t_unequal_pwr, wmw_test_pwr = wmw_pwr))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{raw_data_power}
\end{figure}



\end{document}