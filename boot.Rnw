\documentclass[twoside]{article}

\usepackage[round]{natbib}

\usepackage{amsmath}
\makeatletter
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%If there's more than a single symbol, use the first character instead (see below):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = (italic correction / 3) - (Breite / 10)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern (-\dimen@) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother
\newcommand\test[1]{%
$#1{M}$ $#1{A}$ $#1{g}$ $#1{\beta}$ $#1{\mathcal A}^q$
$#1{AB}^\sigma$ $#1{H}^C$ $#1{\sin z}$ $#1{W}_n$}

% \bibpunct{(}{)}{;}{a}{}{,}


\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{2} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
% \renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\arabic{section}}
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles
\titleformat{\subsubsection}[block]{}{\thesubsubsection.}{1em}{} % Change the look of the section titles


\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{On Solving Applied Inference Problems Using Bootstrap and Simulation Models} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

%----------------------------------------------------------------------------------------
%  TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{On Solving Applied Inference Problems Using Bootstrap and Simulation Models}} % Article title

\author{
\large
\textsc{Christopher Peters}\thanks{Christopher P. Peters is Data Scientist, Treehouse Inc., 622 E. Washington Street Suite 240, Orlando, Florida 32801, USA and Master's of Applied Statistics student, Louisiana State University, 161 Martin D. Woodin Hall, LSU Baton Rouge, Louisiana 70803}\\[2mm] % Your name
\normalsize Louisiana State University \\ % Your institution
\normalsize \href{mailto:cpeter9@gmail.com}{cpeter9@gmail.com} % Your email address
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------



<<set-options, include=FALSE, cache=FALSE>>=
opts_chunk$set(dev='tikz', fig.align='center', cache=TRUE, message=FALSE, background='white')
options(replace.assign=TRUE,width=85)
knit_hooks$set(fig=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)})
library(ggplot2)
theme_set(theme_grey(base_size = 9))
@


\begin{document}


\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%  ABSTRACT
%----------------------------------------------------------------------------------------

\newpage
\begin{abstract}

\noindent Much of what young statisticians in training learn in undergraduate and graduate school relies on large-sample asymptotic theory.  Well known statistics are calculated and with a few assumptions a standard error is developed to measure inference reliability.  Unfortunately data are not always sufficiently large such that finite sample properties match those required by large-sample asymptotics.  Additionally a careful examination of assumptions is always required for successful inference.  In some cases common checks of assumptions may not be possible with a high degree of power, especially in cases of  small samples.  One common solution to this problem involves resampling data with replacement to empirically calculate the variance of any statistic.  This solution is generally referred to as bootstrapping.  In addition to its uses for inference, variations on boostrapping methods can be used to simulate data for cross-validation or other purposes.  This article will focus on three applications of boostrapping.  The first is verification of power calculations for a simple experiment.  The second involves inference in the presence of heteroskedasticity.  The third focuses on bootstrapping for cross-validation.

\end{abstract}

%----------------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Verification of power calculations}

Power calculations are generally considered good pre-requisite for any designed experiment.  Such calculations allow researchers to determine reasonable sample size requirements before carrying out an experiment.  Such calculations can help researchers avoid carrying out experiments doomed in the sense of statistical detection before experimentation begins.

The following example takes a look at how power calculations can be carried out and verified by bootstrapping sample data.  In this case a web app company wants to perform an experiment on a business practice.  The company has found a relationship between earning tokens on the site and customer longevity and financial value.  The company decides that it would like to encourage its customers to earn more tokens by sending an email.  However, the company would also like to compare the effect of the email against a control group of customers that did not receive the email.  A two-sample t-test is selected to test for a difference between the mean number of tokens earned in the experiment group and control group.

A major assumption underlying the two-sample t-test is that the two samples from come from normally distributed populations.  When this assumption is violated, distribution-free tests such as the Wilcoxon-Mann-Whitney (WMW or Mann-Witney U) test are commonly 
used \citep{skovlund01}. It is well known that the two-sample t-test is robust to departures from normality when data are large, but how large?

Suppose that we collect data on token earning prior to our experiment.  In this case the data from two randomly selected groups looks as is shown in Figure~\ref{fig:prob_1_raw_data} in Appendix A.  The data are clearly right skewed and look F, Chi-square, or exponentially distributed. While the t-test is usually robust to departures from normality in large samples, it is costly for the company to run large experiments.  Using bootstrapping, we can resample data from the same data stream that will record the effect of the experiment.  This will allow us to take thousands of samples for varying samples sizes in order to compare power and Type I error rates between the two methods.  The two-sample t-test with pooled variances and with separate variances is also analyzed.  By bootstrapping we are granted a look at the robustness of the two-sample t-test for this data with respect to sample size and when it should be preferred to the WMW test.

The standard two-sample t-test is shown below:

\begin{align}
t &= \frac{\widebar{Y_1} - \widebar{Y_2}}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
\end{align}

The WMW statistic is calculated as follow:

\begin{align}
W &= \frac{\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}I(x_i, y_j)}{mn} && \text{where}\\
I(x_i, y_i) &=
  \begin{array}{l}
    1 : x_i > y_j,\\
    0 : otherwise
  \end{array}
\end{align}

The {\it W} statistic as formulated above is an estimator of P$\left[X > Y\right]$.\\


In order to assess power and type 1 error rates by sample size:
\begin{enumerate}
\item Take two random samples of the target data with replacement with for sample sizes $10\ldots 1010$ by 10.
\item Calculate p-value and store
\item Repeat steps 1 and 2 {\it B} times.  In this case 200 bootstrap samples were taken for each sample and 200 p-values were calculated.
\end{enumerate}

To prove the method, Figure~\ref{type1error} shows type 1 error rate for two random samples from a normal distribution with mean $=$ 4 and standard deviations of 1 and 3, respectively for sample a and sample b bootstraps.  The error rates for the t-test procedure assuming equal variances is exactly the same as that not assuming equal variances.  These error rates are centered around 0.05 as expected.  However, interestingly, these error rates do not appear to be exact nor depend on sample size.  The WMW test maintains an almost universally higher Type 1 error rate compared to the two t-tests except in samples of less than 30.

Power can also be assessed by boostrapping.  The bootstrapping procedure was modified to select two random samples: one from a normal distribution with mean of 4 and standard deviation of 1, and a second sample from a normal distribution with mean 4.4 (10\% higher) and a standard deviation of 3.  Figure~\ref{prob_1_power} shows the share of bootstrapped samples for which the null hypothesis of no difference was rejected for each test by sample size.  Power is low for all tests starting with a sample size of 10.  As the selected sample sizes grow, power increases for all three tests with both variations of the t-test showing identical power.  As sample sizes grow larger than 100, t-tests noticably out perform the WMW test.  Interestingly, power between the t-tests and the WMW test appears to converge around sample size 900 (observations).

Finally, both bootstrapping analyses are performed on the actual token earning data from the company.  Since this data is not normally distributed, it violates a necessary condition for using t-tests in small samples.  Figure~\ref{raw_data_type_1} shows type 1 error rates for each test applied to 200 bootstrap samples at each sample size.  Both t-tests exhibit lower type 1 error rates in samples smaller than about 300 observations compared to the WMW test.  However as sample size increases the WMW test exhibits lower type 1 error rates compared to the two t-tests.  Also, the t-test respecting unequal variances under performs that assuming equal variances in samples of less than about 110 observations.

Power calculations were also performed for all three test variations on the company's data.  Token data was randomly selected for each sample group.  One sample group was left as-is while the other was multiplied by 1.1 to increase the mean 10\%.  This level of increase is motivated by a meaningful difference in token earning as suggested by company representatives.  The WMW test vastly outperformed the two-sample t-tests.  The two variations of t-test performed identically.  By sample size of about 250 observations, the WMW test rejects the null hypothesis in 100\% of the bootstrap samples.  However, both t-tests have only rejected the null hypothesis about 15\% of the time.

% 
% The results in Figure~\ref{prob_1_data} in Appendix A show that the mean differences are indeed normally distributed.  The histogram of mean differences appears to visually approximate a normal distribution.  Increasing the number of bootstrap samples taken causes the histogram to converge to normal with no recognizable difference.  Therefore, the assumption of normality is met in this case of two-sample t-test.\\
% 
% Many statistical tools such as analysis of variance and linear regression require assumptions of normality.  If these assumptions are not met these tools are not valid and may be biased without further transformation of data or adjustments to standard errors.  Bootstrapping is a simple way, often overlooked, to check distributional assumptions.  The general procedure is simple:\\
% 
% \begin{enumerate}
% \item Randomly sample data of interest with replacement B times (generally 1,000 samples is enough).
% \item Calculate and save the statistic of interest for each sample.
% \item Look at the histogram of that statistic.  If it's normally distributed, calculate the standard deviation of the statistic across all samples to obtain the standard error of the statistic.
% \end{enumerate}
% 
% Another assumption requred for the two-sample t-test to be valid is that the standard deviation of the differences is $\chi^{2}$ distributed with posi
% 
% One simple tool for power calculations is the~\href{http://cran.r-project.org/web/packages/pwr/index.html}{pwr} package found in the R language.



\section{Using bootstrapping to test for normality in small samples}

Normality is a common assumption across many statistical procedures such as Analysis of Variance (ANOVA), linear regression, t-tests and z-tests.  When normality assumptions are violated, statistics that make up the aformentioned methods are not valid and may leader to incorrect inference.  There are two main ways that normality is commonly assessed: graphical methods, including histograms plotted against densitites and Q-Q plots.  The second method invovles the use of one of a number of statistical tests for normality.

Small sample sizes typically pose problems for statistical tests of normality.  The major challenge is that such tests have lowre power in small samples and also that small samples may exhibit properities that fool tests for normlaity.  Type 1 error rates, that is rejecting the null hypothesis that the sample data were collected from a population with a normal distribution, may become unstable in small sample sizes.

This paper evaluates four major statistical tests for normality.  The tests include the Shapiro-Wilk test, Kolmogorov-Smirnov test, the Anderson-Darling test and Jarque-Bera test.  The analysis relies on monte carlo simulation as well as bootstrapping across sample sizes.

\subsection{Tests for Normality}

The literature claims that as of 1998 at least 40 different published tests for normality existed~\citep{dufour2008simulation}.  Though estimates seem rather inprecise, there is no doubt in the young statistical practictioner's mind that choosing the correct test for normality can seem daunting.  The mechanics of each test vary significantly and use of each test is usually left to ``rules of thumb'' when passed from the High Priests of statistics to newcomers.  A theme in the choice of test for normality generally seems to be that which was a given statistician's advisor's favorite test.  Rules of thumb generally say, avoid using tests for normality in small samples, but how small?  The proverb that normality tests can miss large departures from normality is common.  In order to assess these questions, deconstruction of each of four tests is followed by monte carlo simulation and bootstrapping across sample sizes.  Rejection rates and Type 1 errors are of a special consideration.

\subsubsection{Shapiro-Wilk}

The Shapiro-Wilk test (SW) for normality was developed and published in 1965~\citep{1965}.  The test was introduced with a limited amount of Monte Carlo simulation analysis itself.  The publishers of the statistic observed power ranging in sample sizes from 3 to 50 observations.  This is somewhat interesting given the common refrain that tests for normality should not be used to assess small samples.  The derivation of the statistic is as follows:  Let $m' = (m_1, M_2,\dots,m_m)$ denote a vector of expected values from a standard normal order statistic, and let $V = (v_ij)$ be the corresponding coveriance matrix.  Allow mean and variance to depend on the sample data for which one hopes to test for departures from normality.  Now let $y' = (y_1,\dots,y_m)$ denote a choice sample for testing.  Under the null hypothesis that the data are indeed normal, it follows that they may be expressed as $y_i = \mu + \sigma x_i$.  This representation is the best linear unbiased estimator of $\sigma$.  Further derivation in~\cite{shapiro1965analysis} shows:

\begin{align}
\widehat{\mu} &= \frac{1}{n}\sum_{1}^{n}y_i = \widebar{y}
\end{align}

and
\begin{align}
\widehat{\sigma} &= \frac{m'V^{-1}y}{m'V^{-1}m}
\end{align}

and let
\begin{align}
S^2 &= \sum_{1}^{n}(y_i - \widehat{y})^2
\end{align}

Then {\it W} can be defined as:

\begin{align}
W &= \frac{(\sum_{i=1}^{n}a_i y_i)^2}{\sum_{i=1}^{n}(y_i - \widebar{y})^2}
\end{align}

where

\begin{align}
a' &= (a_1,\dots,a_m) = \frac{m'V^{-1}}{(m'V^{-1}V^{-1}m)^{1 / 2}}
\end{align}

The complete derivation involves introducing sample variance $S^2$ into a ratio that in essence compares that value to expected variance as scaled by sample data.  The test statistic {\it W} ranges between zero and one.  Values close to zero lead to rejection of the null hypothsis of normality and values closer to one fail to lead to such rejection.  Shapiro-Wilk was later modified by Royston who expanded the statistical procedures to be robust for samples ranging from three to 5,0000 in sample size.~\citep{royston1982extension}

\subsubsection{Kolmogorov-Smirnov Test}

While the Shaprio-Wilk test comes from a class of tests called correlation tests, the Kolmogorov-Smirnov test (KS) from a class called goodness-of-fit tests based on empirical distribution functions (EDF tests).  These tests are so-called because they compare the empirical cumulative density function $F_n(x)$ against the theoretical CDF $F(x)$.  One drawback of such tests is that they are known to have weak power by where the tests require estimation of parameters from the test sample~\citep{1974}.

The Kolmogorov-Sirnov procedure is as follow:

\begin{enumerate}
\item  Sort a sample in ascending order such as $x_1 \le x_2 \le \dots \le x_n$.
\item  Estimate parameters, in this case the mean and variance of the data.
\item Calculate $z_i = F(x_i)$, $i  = 1, 2, \dots, n$, where $F(x)$ is the theoretical normal distribution made empirical by using estimated parameters.
\item Take the supremum absolute value difference between the empirical and theoretical distributions.
\end{enumerate}

The test statistic is as follows:
\begin{align}
T &= sup_x|F(x) - F_n(x)|
\end{align}

\subsubsection{Anderson-Darling Test}

The Anderson-Darling test (AD) is another EDF-type test.  The test is a variant of the popular Cramer-von Mises (CVM) test~\citep{farrell2006comprehensive}.  The AD test statistic is of the form $\int_{-\infty}^{\infty}[F_n(x) - \phi(x)]^2\psi(t)$d{\it t}, where $\psi(t)$ is a weight function of the Cramer-von Mises class.  The weight function is what specifically differentiates the AD test from the CVM test.  It is of the form $\psi = [(\phi(x))(1 - \phi(x))]^{-1}$.  This variate of weight function allows the AD test to place greater importance on differences at the tails of the sample distribution.  The AD test can be computed as:

\begin{align}
\text{AD} &= \left(\frac{1}{n}\right)\sum_{i=1}^{n}(2i - 1)\text{log}[z_i + \text{log}(1 - z_{n+1-i})] - n
\end{align}

where $z_i = \phi(y_{(i)})$ with $y_{(i)} = (x_{(i)} - \widebar{x}) / s$.

\subsubsection{Jarque-Bera}

The Jarque-Bera test, formalized in 1987, is considered part of third type of statistics that test for departures from normality.  These tests are called Moment tests and focus on combinations of sample moments as compares to those of the theoretical normal distribution.  The Jarque-Bera test is given as follows:

\begin{align}
\text{JB} &= n \left(\frac{(\sqrt(b_1))^2}{6} + \frac{(b_2 - 3)^2}{24}\right)
\end{align}

The Jarque-Bera test is very simple to compute and measures sample kurtosis and skewness against theoretical kurtosis and skewness from a normal distribution.

\subsubsection{Monte Carlo simulation}

Comparisons of the four test statistics for power and Type 1 error are made by way of Monte Carlo simulation bootstrapped by sample size.  Six hypothetical distributions of random variables were chosen for the analysis and appear in Figure~\ref{dists} in Appendix A.  The distributions include three normal distributions: one standard normal, one normal distribution $(\mu = 0,~ \sigma^2 = 1)$ with wide variance ($\mu = 0,~\sigma^2 = 3$), and one normal distribution with smaller than standard variance $(\mu = 0,~\sigma^2 = 0.25)$.  Three non-normal distributions are also examined: one uniform distribution $[0,~1]$, one $\chi^2$ distribution $(df = 3)$ and a gamma distribution $(\text{shape} = 2,~\text{scale} = 2)$.

Tests for normality setup and test the hypothesis test below:

\begin{align}
H_0&: \text{The sample comes from a normally distributed population}
H_A&: \text{The sample comes from a population that is not normally distributed}
\end{align}

Unfortunately, many practitioners neglect the significance of normality assumed as the null hypothesis.  Tests of normality actually test for departures from normality.  Therefore normality is never ``proved'' or accepted as the null hypothesis is never accepted.  Contrary, the alternative hypothesis, only a departure from normality can be accepted statistically.  This confusion leads many young practioners to apply their favorite test, note that the probability value is less than 0.05 and continue on with Analysis of Variance or linear regression, etc.  Not only do many (especially young) practitioners reverse the hypotheses, but if they do setup the hypotheses correctly, the second error of claiming to have ``proved'' normality is common.

Type 1 error rates are special consideration in this analysis.  Type 1 error occurs when a practitioner rejects the null hypothesis when the hypothesis is indeed true.  In this case, that would mean that the underlying population distribution is normally distributed, but the practitioner is lead to reject the null hypothesis and accept the alternative hypothesis that the sample came from a population that was not normally distributed.

Monte Carlo simulation can be used to measure Type 1 error rates by drawing random numbers known to be normally distributed.  In this analysis 1,000 separate samples are collected for each distribution.  The sample sizes range from 10 to 1,000 in increments of 5.  In total 792,000 samples are taken with 401,980,000 simulated observations.  For Type 1 error rate analysis we focus on the numbers generated from known normal distributions.  As mentioned above, there are three such classes generated: standard normal, normal with wide variance, and normal with thin variance.  By looking at the share of cases where the null hypothesis is rejected by each test by each sample size, we can calculate Type 1 error rates.

Figure~\ref{type1_monte_carlo} shows Type 1 error rates by test by sample size.  The Shapiro-Wilk, Kolmogrov-Smirnov, and Anderson-Darling tests all appear to have Type 1 error rates that are well controlled around 5\% (as specified).  The Jarque-Bera test's Type 1 error rates are surprisingly lower than 5\% in samples smaller than 50 observations.  The JB Type 1 error rates increase from 10 through about 75 observations at which point they flatten out around 4\%.  This is perhaps not as surprising as it may seem as the Jarque-Bera is a relatively simple test, seemingly constructed more for ease of use rather than precise error control.

Statistical power is a second important concern when it comes to administering tests for departures from normality.  Power, also called the inverse of Type II error rate is the ability of a statistical test to reject the null hypothesis in the presence of a state that in which the null hypothesis should be rejected.  Statistical power is also refered as the ability of a test to detect that which is represented by the alternative hypothesis.  Figure~\ref{power_monte_carlo} shows statistical power for each statistical test by sample size.

Statistical tests for departure from normality were assessed over three distributions: chi-square, gamma, and uniform.  The Shapiro-Wilk test appears to have the best power among the three tests for use on the chi-square and gamma distribution.  The Kolmolgrov-Smirnov test has nearly perfect power against the uniform distribution from sample size 10.  The Jaruq-Bera test presents results that are expected against the chi-square and gamma distributions, but has a strange binomial swing in power as applied to the uniform distribution.  In all three tests, the Shapiro-Wilk test appears to dominate the Anderson-Darling test.

The Shapiro-Wilk test appears to be the majority winner among the three distributions that was applied against each test.  The test reaches nearly pefect power against the chi-square around 50 observations and around 75 against the gamma distribution.

Interestingly, the Kolmogrov-Smirnov test appears to have very low power against the chi-square and gamma distributions as specified, but nearly perfect power against the uniform distribution.  The KS test is noted to have relativley low power in the literature.



\section{Bootstrapping for cross-validation}


\bibliography{boot_biblio}{}
\bibliographystyle{plainnat}

\newpage
\section{Appendix A}

\begin{figure}[h!]
<<fig_1, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='3in', out.height='3in', cache=FALSE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")


sample_a <- sample(table(data$user_id), 400, replace = TRUE)
sample_b <- sample(table(data$user_id), 400, replace = TRUE)

raw.samples <- as.data.frame(t(rbind(sample_a, sample_b)))
raw.samples <- melt(raw.samples)

ggplot(raw.samples, aes(x = value)) +
  geom_histogram(aes(y = ..density..)) +
  facet_grid(variable ~ . )
@
\caption{test caption}
\label{fig:prob_1_raw_data}
\end{figure}

\begin{figure}[h!]
<<type1error, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j
for(i in 1:B){
  
  sample_a <- rnorm(sample.size, mean = 4, sd = 1)
  sample_b <- rnorm(sample.size, mean = 4, sd = 3)
  
  sample_c <- rnorm(sample.size, mean = 4, sd = 1)
  sample_d <- rnorm(sample.size, mean = 4 * 1.1, sd = 3) # thirty percent increase in mean

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
  
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test = t_type_1, t_test_unequal = t_unequal_type_1, wmw_test = wmw_type_1))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{type1error}
\end{figure}


\begin{figure}[h!]
<<power, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j
for(i in 1:B){
  
  sample_a <- rnorm(sample.size, mean = 4, sd = 1)
  sample_b <- rnorm(sample.size, mean = 4, sd = 3)
  
  sample_c <- rnorm(sample.size, mean = 4, sd = 1)
  sample_d <- rnorm(sample.size, mean = 4 * 1.1, sd = 3) # thirty percent increase in mean

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
  
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test_pwr = t_pwr, t_test_unequal_pwr = t_unequal_pwr, wmw_test_pwr = wmw_pwr))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{prob_1_power}
\end{figure}

\begin{figure}[h!]
<<raw_data_type_1, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j 
for(i in 1:B){
  
  sample_a <- sample(table(data$user_id), sample.size, replace = TRUE)
  sample_b <- sample(table(data$user_id), sample.size, replace = TRUE)

  t_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals[i] <- t.test(sample_a, sample_b, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals[i] <- wilcox.test(sample_a, sample_b, alternative = "two.sided", conf.level = 0.95)$p.value
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_type_1[j/move_by] <- ifelse(is.na(table(t_p_vals <= 0.05)[2]), 0, table(t_p_vals <= 0.05)[2]) / B
t_unequal_type_1[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals <= 0.05)[2]), 0, table(t_unequal_p_vals <= 0.05)[2]) / B
wmw_type_1[j/move_by] <- ifelse(is.na(table(wmw_p_vals <= 0.05)[2]), 0, table(wmw_p_vals <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test = t_type_1, t_test_unequal = t_unequal_type_1, wmw_test = wmw_type_1))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{raw_data_type_1}
\end{figure}

\begin{figure}[h!]
<<raw_data_power, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)

data <- read.csv("C:/R_stuff/school/article/data.csv")

B <- 200
t_p_vals <- rep(NA, B)
t_unequal_p_vals <- rep(NA, B)
wmw_p_vals <- rep(NA, B)
t_p_vals_pwr <- rep(NA, B)
t_unequal_p_vals_pwr <- rep(NA, B)
wmw_p_vals_pwr <- rep(NA, B)

begin <- 10; end <- 1010; move_by <- 100
t_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_type_1 <- rep(NA, length(seq(begin, end, move_by)))
wmw_type_1 <- rep(NA, length(seq(begin, end, move_by)))
t_pwr <- rep(NA, length(seq(begin, end, move_by)))
t_unequal_pwr <- rep(NA, length(seq(begin, end, move_by)))
wmw_pwr <- rep(NA, length(seq(begin, end, move_by)))

for(j in seq(begin, end, move_by)){
  sample.size <- j 
for(i in 1:B){
  
  sample_c <- sample(table(data$user_id), sample.size, replace = TRUE)
  sample_d <- sample(table(data$user_id), sample.size, replace = TRUE) * 1.1
  
  t_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = TRUE, conf.level = 0.95)$p.value
  t_unequal_p_vals_pwr[i] <- t.test(sample_c, sample_d, alternative = "two.sided", var.equal = FALSE, conf.level = 0.95)$p.value
  wmw_p_vals_pwr[i] <- wilcox.test(sample_c, sample_d, alternative = "two.sided", conf.level = 0.95)$p.value
  }
t_pwr[j/move_by] <- ifelse(is.na(table(t_p_vals_pwr <= 0.05)[2]), B, table(t_p_vals_pwr <= 0.05)[2]) / B
t_unequal_pwr[j/move_by] <- ifelse(is.na(table(t_unequal_p_vals_pwr <= 0.05)[2]), B, table(t_unequal_p_vals_pwr <= 0.05)[2]) / B
wmw_pwr[j/move_by] <- ifelse(is.na(table(wmw_p_vals_pwr <= 0.05)[2]), B, table(wmw_p_vals_pwr <= 0.05)[2]) / B
}

output <- as.data.frame(list(t_test_pwr = t_pwr, t_test_unequal_pwr = t_unequal_pwr, wmw_test_pwr = wmw_pwr))
output <- melt(output)
output$sample.size <- seq(begin, end, move_by)

ggplot(output, aes(x = sample.size, y = value, colour = variable)) +
  geom_smooth() +
  geom_point()
@
\caption{test caption}
\label{raw_data_power}
\end{figure}


\begin{figure}[h!]
<<dists, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
library(ggplot2)
library(reshape2)
library(tikzDevice)
sample.size <- 10000

# Various normal distributions
normal <- rnorm(sample.size, mean = 0, sd = 1)
normal_wide <- rnorm(sample.size, mean = 0, sd = 3)
normal_skinny <- rnorm(sample.size, mean = 0, sd = 0.5)

# Other distributions
uniform <- runif(sample.size, min = 0, max = 1)
chisq <- rchisq(sample.size, df = 3)
gamma <- rgamma(sample.size, shape = 2, rate = 2)

dists <- as.data.frame(list(normal = normal, normal_wide = normal_wide, normal_skinny = normal_skinny,
                   uniform = uniform, chisq = chisq, gamma = gamma))

dists <- melt(dists)

ggplot(dists, aes(x = value)) +
  geom_density(fill = "blue", alpha = 0.5) +
  scale_x_continuous(limits = c(-10, 10)) +
  facet_wrap( ~ variable) +
stat_function(fun = dnorm,
              args = c(mean = 0,
                       sd = 1),
              colour = "red", size = 2)
@
\caption{test caption}
\label{dists}
\end{figure}


\begin{figure}[h!]
<<type1, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
# Loop over small sample sizes
library(ggplot2)
library(reshape2)
library(tikzDevice)
library(nortest)
library(tseries)


smallest.size <- 10
largest.size <- 100
size.by <- 5
boot.size <- 1000

for(i in c(".sw", ".ks", ".ad", ".jb")){ # create fill vectors to put bootstapped p-values in
  norm <- rep(NA, boot.size)
  norm.wide <- rep(NA, boot.size)
  norm.skinny <- rep(NA, boot.size)
  uniform <- rep(NA, boot.size)
  chisq <- rep(NA, boot.size)
  gamma <- rep(NA, boot.size)
  
  if(i == ".sw") {p_fill <- as.data.frame(cbind(norm, norm.wide, norm.skinny, uniform, chisq, gamma))
                 names(p_fill) <- paste(names(p_fill), i, sep = "")} else
                 {temp <- as.data.frame(cbind(norm, norm.wide, norm.skinny, uniform, chisq, gamma))
                  names(temp) <- paste(names(temp), i, sep = "")
                  p_fill <- cbind(p_fill, temp)}
}


for(i in seq(smallest.size, largest.size, by = size.by)){
  sample.size <- i
  
  for(j in 1:boot.size){
  # Various normal distributions
  normal <- rnorm(sample.size, mean = 0, sd = 1)
  normal_wide <- rnorm(sample.size, mean = 0, sd = 3)
  normal_skinny <- rnorm(sample.size, mean = 0, sd = 0.5)
  
  # Other distributions
  uniform <- runif(sample.size, min = 0, max = 1)
  chisq <- rchisq(sample.size, df = 3)
  gamma <- rgamma(sample.size, shape = 2, rate = 2)
  
  # SW test
  p_fill$norm.sw[j] <- shapiro.test(normal)$p.value
  p_fill$norm.wide.sw[j] <- shapiro.test(normal_wide)$p.value
  p_fill$norm.skinny.sw[j] <- shapiro.test(normal_skinny)$p.value
  p_fill$uniform.sw[j] <- shapiro.test(uniform)$p.value
  p_fill$chisq.sw[j] <- shapiro.test(chisq)$p.value
  p_fill$gamma.sw[j] <- shapiro.test(gamma)$p.value
  
  # KS test
  p_fill$norm.ks[j] <- ks.test(normal, "pnorm", alternative = "two.sided", mean = 0, sd = 1)$p.value
  p_fill$norm.wide.ks[j] <- ks.test(normal_wide, "pnorm", alternative = "two.sided", mean = 0, sd = 3)$p.value
  p_fill$norm.skinny.ks[j] <- ks.test(normal_skinny, "pnorm", alternative = "two.sided", mean = 0, sd = 0.5)$p.value
  p_fill$uniform.ks[j] <- ks.test(uniform, "pnorm", alternative = "two.sided", mean = mean(uniform), sd = sd(uniform))$p.value
  p_fill$chisq.ks[j] <- ks.test(chisq, "pnorm", alternative = "two.sided", mean = mean(chisq), sd = sd(chisq))$p.value
  p_fill$gamma.ks[j] <- ks.test(gamma, "pnorm", alternative = "two.sided", mean = mean(gamma), sd = sd(gamma))$p.value
  
  # AD test
  p_fill$norm.ad[j] <- ad.test(normal)$p.value
  p_fill$norm.wide.ad[j] <- ad.test(normal_wide)$p.value
  p_fill$norm.skinny.ad[j] <- ad.test(normal_skinny)$p.value
  p_fill$uniform.ad[j] <- ad.test(uniform)$p.value
  p_fill$chisq.ad[j] <- ad.test(chisq)$p.value
  p_fill$gamma.ad[j] <- ad.test(gamma)$p.value
  
  # AD test
  p_fill$norm.jb[j] <- jarque.bera.test(normal)$p.value
  p_fill$norm.wide.jb[j] <- jarque.bera.test(normal_wide)$p.value
  p_fill$norm.skinny.jb[j] <- jarque.bera.test(normal_skinny)$p.value
  p_fill$uniform.jb[j] <- jarque.bera.test(uniform)$p.value
  p_fill$chisq.jb[j] <- jarque.bera.test(chisq)$p.value
  p_fill$gamma.jb[j] <- jarque.bera.test(gamma)$p.value
  }
  
  if(sample.size == smallest.size) {
    temp <- as.data.frame(cbind(sample.size = sample.size,
                                p_fill))} else {
                                 temp <- rbind(temp, cbind(sample.size, p_fill))}
}

output <- melt(temp, "sample.size")
output$variable <- as.character(output$variable)

# seperate.tests
substrRight <- function(x, n){
  substr(x, nchar(x) - n + 1, nchar(x))
}

substrLeft <- function(x, n){
  substr(x, 1, nchar(x) - n)
}

output$test <- substrRight(output$variable, 2)
output$dist <- substrLeft(output$variable, 3)

df <- aggregate(output$value > 0.05, list(output$sample.size, output$dist, output$test), table)

df$rejected <- rep(NA, length(df$x))
df$not_rejected <- rep(NA, length(df$x))
for(i in 1:length(df$x)){
  df$rejected[i] <- unlist(df[ , "x"][i])[1]
  df$not_rejected[i] <- unlist(df[ , "x"][i])[2]
}

df$rejected <- ifelse(is.na(df$rejected), 0, df$rejected)
df$not_rejected <- ifelse(is.na(df$not_rejected), 0, df$not_rejected)
df$power_type1 <- with(df, rejected / (rejected + not_rejected))
    names(df) <- c("sample.size", "dist", "test", "x", "rejected", "not_rejected", "power_type1")

ggplot(df[grep("norm", df$dist), ], aes(x = sample.size, y = power_type1, colour = factor(dist))) +
  geom_smooth() +
  facet_wrap(~ test)
@
\caption{test caption}
\label{type1_monte_carlo}
\end{figure}

\begin{figure}[h!]
<<power_test, echo=FALSE, fig=TRUE, sanitize=TRUE, out.width='5in', out.height='3in', warning=FALSE, cache=TRUE>>=
# Loop over small sample sizes
library(ggplot2)
library(reshape2)
library(tikzDevice)
library(nortest)
library(tseries)


smallest.size <- 10
largest.size <- 100
size.by <- 5
boot.size <- 1000

for(i in c(".sw", ".ks", ".ad", ".jb")){ # create fill vectors to put bootstapped p-values in
  norm <- rep(NA, boot.size)
  norm.wide <- rep(NA, boot.size)
  norm.skinny <- rep(NA, boot.size)
  uniform <- rep(NA, boot.size)
  chisq <- rep(NA, boot.size)
  gamma <- rep(NA, boot.size)
  
  if(i == ".sw") {p_fill <- as.data.frame(cbind(norm, norm.wide, norm.skinny, uniform, chisq, gamma))
                 names(p_fill) <- paste(names(p_fill), i, sep = "")} else
                 {temp <- as.data.frame(cbind(norm, norm.wide, norm.skinny, uniform, chisq, gamma))
                  names(temp) <- paste(names(temp), i, sep = "")
                  p_fill <- cbind(p_fill, temp)}
}


for(i in seq(smallest.size, largest.size, by = size.by)){
  sample.size <- i
  
  for(j in 1:boot.size){
  # Various normal distributions
  normal <- rnorm(sample.size, mean = 0, sd = 1)
  normal_wide <- rnorm(sample.size, mean = 0, sd = 3)
  normal_skinny <- rnorm(sample.size, mean = 0, sd = 0.5)
  
  # Other distributions
  uniform <- runif(sample.size, min = 0, max = 1)
  chisq <- rchisq(sample.size, df = 3)
  gamma <- rgamma(sample.size, shape = 2, rate = 2)
  
  # SW test
  p_fill$norm.sw[j] <- shapiro.test(normal)$p.value
  p_fill$norm.wide.sw[j] <- shapiro.test(normal_wide)$p.value
  p_fill$norm.skinny.sw[j] <- shapiro.test(normal_skinny)$p.value
  p_fill$uniform.sw[j] <- shapiro.test(uniform)$p.value
  p_fill$chisq.sw[j] <- shapiro.test(chisq)$p.value
  p_fill$gamma.sw[j] <- shapiro.test(gamma)$p.value
  
  # KS test
  p_fill$norm.ks[j] <- ks.test(normal, "pnorm", alternative = "two.sided", mean = 0, sd = 1)$p.value
  p_fill$norm.wide.ks[j] <- ks.test(normal_wide, "pnorm", alternative = "two.sided", mean = 0, sd = 3)$p.value
  p_fill$norm.skinny.ks[j] <- ks.test(normal_skinny, "pnorm", alternative = "two.sided", mean = 0, sd = 0.5)$p.value
  p_fill$uniform.ks[j] <- ks.test(uniform, "punif", alternative = "two.sided", min = 0, max = 0)$p.value
  p_fill$chisq.ks[j] <- ks.test(chisq, "pchisq", alternative = "greater", df = 3)$p.value
  p_fill$gamma.ks[j] <- ks.test(gamma, "pgamma", alternative = "greater", shape = 2, rate = 2)$p.value
  
  # AD test
  p_fill$norm.ad[j] <- ad.test(normal)$p.value
  p_fill$norm.wide.ad[j] <- ad.test(normal_wide)$p.value
  p_fill$norm.skinny.ad[j] <- ad.test(normal_skinny)$p.value
  p_fill$uniform.ad[j] <- ad.test(uniform)$p.value
  p_fill$chisq.ad[j] <- ad.test(chisq)$p.value
  p_fill$gamma.ad[j] <- ad.test(gamma)$p.value
  
  # JB test
  p_fill$norm.jb[j] <- jarque.bera.test(normal)$p.value
  p_fill$norm.wide.jb[j] <- jarque.bera.test(normal_wide)$p.value
  p_fill$norm.skinny.jb[j] <- jarque.bera.test(normal_skinny)$p.value
  p_fill$uniform.jb[j] <- jarque.bera.test(uniform)$p.value
  p_fill$chisq.jb[j] <- jarque.bera.test(chisq)$p.value
  p_fill$gamma.jb[j] <- jarque.bera.test(gamma)$p.value
  }
  
  if(sample.size == smallest.size) {
    temp <- as.data.frame(cbind(sample.size = sample.size,
                                p_fill))} else {
                                 temp <- rbind(temp, cbind(sample.size, p_fill))}
}

output <- melt(temp, "sample.size")
output$variable <- as.character(output$variable)

# seperate.tests
substrRight <- function(x, n){
  substr(x, nchar(x) - n + 1, nchar(x))
}

substrLeft <- function(x, n){
  substr(x, 1, nchar(x) - n)
}

output$test <- substrRight(output$variable, 2)
output$dist <- substrLeft(output$variable, 3)

df <- aggregate(output$value > 0.05, list(output$sample.size, output$dist, output$test), table)

df$rejected <- rep(NA, length(df$x))
df$not_rejected <- rep(NA, length(df$x))
for(i in 1:length(df$x)){
  df$rejected[i] <- unlist(df[ , "x"][i])[1]
  df$not_rejected[i] <- unlist(df[ , "x"][i])[2]
}

df$rejected <- ifelse(is.na(df$rejected), 0, df$rejected)
df$not_rejected <- ifelse(is.na(df$not_rejected), 0, df$not_rejected)
df$power_type1 <- with(df, rejected / (rejected + not_rejected))
    names(df) <- c("sample.size", "dist", "test", "x", "rejected", "not_rejected", "power_type1")

ggplot(df[-grep("norm", df$dist), ], aes(x = sample.size, y = power_type1, colour = factor(test))) +
  geom_smooth() +
  geom_point() +
  facet_wrap(~ dist)
@
\caption{test caption}
\label{power_monte_carlo}
\end{figure}

\end{document}